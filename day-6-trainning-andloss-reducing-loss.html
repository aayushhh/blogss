<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Day 5 Trainning &amp;Loss, Reducing Loss  - Blogs</title><meta name="description" content="🕹 Training &amp; Loss 🕹In supervised learning training a model means learning good values for weights &amp; bias from labelled examples. In doing so it attempts to find a model that minimises loss. Process is called empirical risk minimisation 📌 Loss indicates how bad model's prediction was on an example📌 Loss = 0 if the model's prediction is perfect otherwise it's greater.📌 Goal of training is to find weights &amp; biases that have low loss, on average, across data setSquared loss or L2 loss is a popular Loss function that aggregates the individual lossesL2 loss for a single example = (observation - prediction)^2Mean square error (MSE) is the average squared loss per example over the whole data set. To calculate MSE, sum up all the squared losses for individual examples and then divide by the number of examples. It looks like this 👇Here is a fun read on loss function by @quaesita 👉 bit.ly/quaesita_emper… Machine learning — Is the emperor wearing clothes? | Hacker Noonhttps://bit.ly/quaesita_emperor"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://aayushhh.github.io/blogss/day-6-trainning-andloss-reducing-loss.html"><link rel="alternate" type="application/atom+xml" href="https://aayushhh.github.io/blogss/feed.xml"><link rel="alternate" type="application/json" href="https://aayushhh.github.io/blogss/feed.json"><meta property="og:title" content="Day 5 Trainning &Loss, Reducing Loss "><meta property="og:image" content="https://aayushhh.github.io/blogss/media/posts/6/72f1695b-f430-4dfc-a786-edf49bfda810_200x200.png"><meta property="og:site_name" content="Blogs"><meta property="og:description" content="🕹 Training &amp; Loss 🕹In supervised learning training a model means learning good values for weights &amp; bias from labelled examples. In doing so it attempts to find a model that minimises loss. Process is called empirical risk minimisation 📌 Loss indicates how bad model's prediction was on an example📌 Loss = 0 if the model's prediction is perfect otherwise it's greater.📌 Goal of training is to find weights &amp; biases that have low loss, on average, across data setSquared loss or L2 loss is a popular Loss function that aggregates the individual lossesL2 loss for a single example = (observation - prediction)^2Mean square error (MSE) is the average squared loss per example over the whole data set. To calculate MSE, sum up all the squared losses for individual examples and then divide by the number of examples. It looks like this 👇Here is a fun read on loss function by @quaesita 👉 bit.ly/quaesita_emper… Machine learning — Is the emperor wearing clothes? | Hacker Noonhttps://bit.ly/quaesita_emperor"><meta property="og:url" content="https://aayushhh.github.io/blogss/day-6-trainning-andloss-reducing-loss.html"><meta property="og:type" content="article"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@aayushhh"><meta name="twitter:title" content="Day 5 Trainning &Loss, Reducing Loss "><meta name="twitter:description" content="🕹 Training &amp; Loss 🕹In supervised learning training a model means learning good values for weights &amp; bias from labelled examples. In doing so it attempts to find a model that minimises loss. Process is called empirical risk minimisation 📌 Loss indicates how bad model's prediction was on an example📌 Loss = 0 if the model's prediction is perfect otherwise it's greater.📌 Goal of training is to find weights &amp; biases that have low loss, on average, across data setSquared loss or L2 loss is a popular Loss function that aggregates the individual lossesL2 loss for a single example = (observation - prediction)^2Mean square error (MSE) is the average squared loss per example over the whole data set. To calculate MSE, sum up all the squared losses for individual examples and then divide by the number of examples. It looks like this 👇Here is a fun read on loss function by @quaesita 👉 bit.ly/quaesita_emper… Machine learning — Is the emperor wearing clothes? | Hacker Noonhttps://bit.ly/quaesita_emperor"><meta name="twitter:image" content="https://aayushhh.github.io/blogss/media/posts/6/72f1695b-f430-4dfc-a786-edf49bfda810_200x200.png"><style>:root{--body-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--heading-font:var(--body-font);--post-entry-font:var(--body-font);--logo-font:var(--body-font);--menu-font:var(--body-font)}</style><link rel="stylesheet" href="https://aayushhh.github.io/blogss/assets/css/style.css?v=eacb492672ba3c1641d647982de266ba"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://aayushhh.github.io/blogss/day-6-trainning-andloss-reducing-loss.html"},"headline":"Day 5 Trainning &Loss, Reducing Loss ","datePublished":"2021-03-06T12:55","dateModified":"2021-03-06T13:26","image":{"@type":"ImageObject","url":"https://aayushhh.github.io/blogss/media/posts/6/72f1695b-f430-4dfc-a786-edf49bfda810_200x200.png","height":200,"width":200},"description":"<h2 class=\"align-center\"><strong>🕹 Training &amp; Loss 🕹</strong></h2>\n<p><span style=\"font-weight: 400;\">In supervised learning training a model means learning good values for weights &amp; bias from labelled examples. In doing so it attempts to find a model that minimises loss. Process is called empirical risk minimisation</span></p>\n<p><span style=\"font-weight: 400;\"> </span></p>\n<p><span style=\"font-weight: 400;\">📌 Loss indicates how bad model's prediction was on an example</span></p>\n<p><span style=\"font-weight: 400;\">📌 Loss = 0 if the model's prediction is perfect otherwise it's greater.</span></p>\n<p><span style=\"font-weight: 400;\">📌 Goal of training is to find weights &amp; biases that have low loss, on average, across data set</span></p>\n<hr>\n<figure class=\"post__image post__image--center\"><img loading=\"lazy\"  src=\"https://aayushhh.github.io/blogss/media/posts/6/Screenshot-from-2021-03-06-12-51-16.png\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-51-16-xs.png 300w ,https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-51-16-sm.png 480w ,https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-51-16-md.png 768w ,https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-51-16-lg.png 1024w ,https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-51-16-xl.png 1360w ,https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-51-16-2xl.png 1600w\"  alt=\"\" width=\"688\" height=\"355\"></figure>\n<p><span style=\"font-weight: 400;\">Squared loss or L2 loss is a popular Loss function that aggregates the individual losses</span></p>\n<p><span style=\"font-weight: 400;\">L2 loss for a single example = (observation - prediction)^2</span></p>\n<hr>\n<p class=\"dropcap\"><span style=\"font-weight: 400;\"><strong>Mean square error (MSE)</strong> is the average squared loss per example over the whole data set. To calculate MSE, sum up all the squared losses for individual examples and then divide by the number of examples. It looks like this 👇</span></p>\n<figure class=\"post__image post__image--center\"><img loading=\"lazy\"  src=\"https://aayushhh.github.io/blogss/media/posts/6/Screenshot-from-2021-03-06-12-51-36.png\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-51-36-xs.png 300w ,https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-51-36-sm.png 480w ,https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-51-36-md.png 768w ,https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-51-36-lg.png 1024w ,https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-51-36-xl.png 1360w ,https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-51-36-2xl.png 1600w\"  alt=\"MSE\" width=\"689\" height=\"372\"></figure>\n<p><span style=\"font-weight: 400;\">Here is a fun read on loss function by </span><a href=\"https://twitter.com/quaesita\"><span style=\"font-weight: 400;\">@quaesita</span></a><span style=\"font-weight: 400;\"> 👉 </span><a href=\"https://bit.ly/quaesita_emperor\"><span style=\"font-weight: 400;\">bit.ly/quaesita_emper…</span></a></p>\n<p> </p>\n<p><a href=\"https://hackernoon.com/machine-learning-is-the-emperor-wearing-clothes-59933d12a3cc\"><span style=\"font-weight: 400;\">Machine learning — Is the emperor wearing clothes? | Hacker Noon</span></a></p>\n<p><a href=\"https://hackernoon.com/machine-learning-is-the-emperor-wearing-clothes-59933d12a3cc\"><span style=\"font-weight: 400;\">https://bit.ly/quaesita_emperor</span></a></p>\n","author":{"@type":"Person","name":"Aayush"},"publisher":{"@type":"Organization","name":"Aayush"}}</script></head><body><header class="topbar" id="js-header"><div class="topbar__inner"><a class="logo" href="https://aayushhh.github.io/blogss/">Blogs</a></div></header><div class="content"><div class="infobar"><div class="infobar__update">Last Updated: <time datetime="2021-03-06T13:29">March 6, 2021</time></div><div class="infobar__search"><form action="https://aayushhh.github.io/blogss/search.html"><input type="search" name="q" placeholder="search..." aria-label="Search input"> <button type="submit" aria-label="Submit"><svg role="presentation" focusable="false" width="15px" height="14px"><use xlink:href="https://aayushhh.github.io/blogss/assets/svg/svg-map.svg#search"/></svg></button></form></div></div><main class="main"><article class="post"><figure class="post__featured-image"><img src="https://aayushhh.github.io/blogss/media/posts/6/72f1695b-f430-4dfc-a786-edf49bfda810_200x200.png" srcset="https://aayushhh.github.io/blogss/media/posts/6/responsive/72f1695b-f430-4dfc-a786-edf49bfda810_200x200-xs.png 300w, https://aayushhh.github.io/blogss/media/posts/6/responsive/72f1695b-f430-4dfc-a786-edf49bfda810_200x200-sm.png 480w, https://aayushhh.github.io/blogss/media/posts/6/responsive/72f1695b-f430-4dfc-a786-edf49bfda810_200x200-md.png 768w, https://aayushhh.github.io/blogss/media/posts/6/responsive/72f1695b-f430-4dfc-a786-edf49bfda810_200x200-lg.png 1024w, https://aayushhh.github.io/blogss/media/posts/6/responsive/72f1695b-f430-4dfc-a786-edf49bfda810_200x200-xl.png 1360w, https://aayushhh.github.io/blogss/media/posts/6/responsive/72f1695b-f430-4dfc-a786-edf49bfda810_200x200-2xl.png 1600w" sizes="(max-width: 1600px) 100vw, 1600px" loading="eager" height="200" width="200" alt=""></figure><header class="u-header post__header"><h1>Day 5 Trainning &amp;Loss, Reducing Loss </h1><div class="u-header__meta u-small"><div><a href="https://aayushhh.github.io/blogss/authors/aayush/" title="Aayush">Aayush</a> <time datetime="2021-03-06T12:55">March 6, 2021 </time><a href="https://aayushhh.github.io/blogss/tags/training-loss/" class="u-tag u-tag--1">Training Loss</a></div></div></header><div class="post__entry u-inner"><h2 class="align-center"><strong>🕹 Training &amp; Loss 🕹</strong></h2><p><span style="font-weight: 400;">In supervised learning training a model means learning good values for weights &amp; bias from labelled examples. In doing so it attempts to find a model that minimises loss. Process is called empirical risk minimisation</span></p><p><span style="font-weight: 400;"> </span></p><p><span style="font-weight: 400;">📌 Loss indicates how bad model's prediction was on an example</span></p><p><span style="font-weight: 400;">📌 Loss = 0 if the model's prediction is perfect otherwise it's greater.</span></p><p><span style="font-weight: 400;">📌 Goal of training is to find weights &amp; biases that have low loss, on average, across data set</span></p><hr><figure class="post__image post__image--center"><img loading="lazy" src="https://aayushhh.github.io/blogss/media/posts/6/Screenshot-from-2021-03-06-12-51-16.png" sizes="(max-width: 48em) 100vw, 768px" srcset="https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-51-16-xs.png 300w, https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-51-16-sm.png 480w, https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-51-16-md.png 768w, https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-51-16-lg.png 1024w, https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-51-16-xl.png 1360w, https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-51-16-2xl.png 1600w" alt="" width="688" height="355"></figure><p><span style="font-weight: 400;">Squared loss or L2 loss is a popular Loss function that aggregates the individual losses</span></p><p><span style="font-weight: 400;">L2 loss for a single example = (observation - prediction)^2</span></p><hr><p class="dropcap"><span style="font-weight: 400;"><strong>Mean square error (MSE)</strong> is the average squared loss per example over the whole data set. To calculate MSE, sum up all the squared losses for individual examples and then divide by the number of examples. It looks like this 👇</span></p><figure class="post__image post__image--center"><img loading="lazy" src="https://aayushhh.github.io/blogss/media/posts/6/Screenshot-from-2021-03-06-12-51-36.png" sizes="(max-width: 48em) 100vw, 768px" srcset="https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-51-36-xs.png 300w, https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-51-36-sm.png 480w, https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-51-36-md.png 768w, https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-51-36-lg.png 1024w, https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-51-36-xl.png 1360w, https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-51-36-2xl.png 1600w" alt="MSE" width="689" height="372"></figure><p><span style="font-weight: 400;">Here is a fun read on loss function by </span><a href="https://twitter.com/quaesita"><span style="font-weight: 400;">@quaesita</span></a><span style="font-weight: 400;"> 👉 </span><a href="https://bit.ly/quaesita_emperor"><span style="font-weight: 400;">bit.ly/quaesita_emper…</span></a></p><p> </p><p><a href="https://hackernoon.com/machine-learning-is-the-emperor-wearing-clothes-59933d12a3cc"><span style="font-weight: 400;">Machine learning — Is the emperor wearing clothes? | Hacker Noon</span></a></p><p><a href="https://hackernoon.com/machine-learning-is-the-emperor-wearing-clothes-59933d12a3cc"><span style="font-weight: 400;">https://bit.ly/quaesita_emperor</span></a></p><p> </p><h2 class="align-center"><span style="font-weight: 400;">⬇️ Reducing Loss ⬇️</span></h2><p><span style="font-weight: 400;">An iterative process of choosing model parameters that minimise loss</span></p><p><span style="font-weight: 400;">👉 Loss function is how we compute loss</span></p><p><span style="font-weight: 400;">👉 Loss function curve is convex for linear regression</span></p><figure class="post__image post__image--center"><img loading="lazy" src="https://aayushhh.github.io/blogss/media/posts/6/Screenshot-from-2021-03-06-12-54-04.png" sizes="(max-width: 48em) 100vw, 768px" srcset="https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-54-04-xs.png 300w, https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-54-04-sm.png 480w, https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-54-04-md.png 768w, https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-54-04-lg.png 1024w, https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-54-04-xl.png 1360w, https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-54-04-2xl.png 1600w" alt="Reducing Loss" width="471" height="322"><figcaption>Caption</figcaption></figure><p> </p><p><span style="font-weight: 400;">Calculating loss for every value of W isn't efficient: most common way is called gradient descent</span></p><p><span style="font-weight: 400;">👉 Start with any value of w, b (weights &amp; biases)</span></p><p><span style="font-weight: 400;">👉 Keep going until overall loss stops changing or changes slowly</span></p><p><span style="font-weight: 400;">👉 That point is called convergence</span></p><hr><figure class="post__image post__image--center"><img loading="lazy" src="https://aayushhh.github.io/blogss/media/posts/6/Screenshot-from-2021-03-06-12-54-19.png" sizes="(max-width: 48em) 100vw, 768px" srcset="https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-54-19-xs.png 300w, https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-54-19-sm.png 480w, https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-54-19-md.png 768w, https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-54-19-lg.png 1024w, https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-54-19-xl.png 1360w, https://aayushhh.github.io/blogss/media/posts/6/responsive/Screenshot-from-2021-03-06-12-54-19-2xl.png 1600w" alt="" width="690" height="359"></figure><p class="msg msg--success"><span style="font-weight: 400;">As you probably already guessed, gradient is a vector with:</span></p><p><span style="font-weight: 400;">👉 Direction</span></p><p><span style="font-weight: 400;">👉 Magnitude</span></p><p class="msg msg--highlight"><span style="font-weight: 400;">Gradient descent algorithms multiply the gradient by a scalar known as the learning rate (or step size) to determine the next point.</span></p><p> </p><p><span style="font-weight: 400;">Here are a few resources I have used for this thread:</span></p><p><span style="font-weight: 400;">👉 </span><a href="https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent"><span style="font-weight: 400;">developers.google.com/machine-learni…</span></a></p><p><span style="font-weight: 400;">👉 </span><a href="https://bit.ly/quaesita_emperor"><span style="font-weight: 400;">bit.ly/quaesita_emper…</span></a></p><p> </p><p> </p><p><a href="https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent"><span style="font-weight: 400;">Reducing Loss: Gradient Descent  |  Machine Learning Crash Course</span></a></p><p><a href="https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent"><span style="font-weight: 400;">https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent</span></a></p><p><a href="https://hackernoon.com/machine-learning-is-the-emperor-wearing-clothes-59933d12a3cc"><span style="font-weight: 400;">Machine learning — Is the emperor wearing clothes? | Hacker Noon</span></a></p><p><a href="https://hackernoon.com/machine-learning-is-the-emperor-wearing-clothes-59933d12a3cc"><span style="font-weight: 400;">https://bit.ly/quaesita_emperor</span></a></p><p> </p></div><aside class="post__aside"><div class="post__last-updated u-small">This article was updated on March 6, 2021</div><div class="post__share"><a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Faayushhh.github.io%2Fblogss%2Fday-6-trainning-andloss-reducing-loss.html" class="js-share facebook" title="Share with Facebook" rel="nofollow noopener noreferrer" aria-label="Share with Facebook"><svg class="u-icon"><use xlink:href="https://aayushhh.github.io/blogss/assets/svg/svg-map.svg#facebook"/></svg> </a><a href="https://wa.me/?text=Day%205%20Trainning%20%26Loss%2C%20Reducing%20Loss%C2%A0 https%3A%2F%2Faayushhh.github.io%2Fblogss%2Fday-6-trainning-andloss-reducing-loss.html" class="js-share whatsapp" rel="nofollow noopener noreferrer" title="Share with WhatsApp" aria-label="Share with WhatsApp"><svg class="u-icon"><use xlink:href="https://aayushhh.github.io/blogss/assets/svg/svg-map.svg#whatsapp"/></svg> </a><a href="http://www.linkedin.com/shareArticle?url=https%3A%2F%2Faayushhh.github.io%2Fblogss%2Fday-6-trainning-andloss-reducing-loss.html&amp;title=Day%205%20Trainning%20%26Loss%2C%20Reducing%20Loss%C2%A0" class="js-share linkedin" title="Share with LinkedIn" rel="nofollow noopener noreferrer" aria-label="Share with LinkedIn"><svg class="u-icon"><use xlink:href="https://aayushhh.github.io/blogss/assets/svg/svg-map.svg#linkedin"/></svg></a></div></aside><footer class="post__footer"><a href="https://aayushhh.github.io/blogss/day-6-trainning-andloss-reducing-loss.html#disqus_thread" data-disqus-identifier="6" class="u-comment-count" rel="nofollow">0</a><div class="post__bio box"><div><h4 class="h6"><a href="https://aayushhh.github.io/blogss/authors/aayush/" title="Aayush">Aayush</a></h4></div></div><nav class="post__nav box"><div class="post__nav__prev"><a href="https://aayushhh.github.io/blogss/day-5-unsupervised-learning.html" class="post__nav__link" rel="prev"><img src="https://aayushhh.github.io/blogss/media/posts/5/responsive/72f1695b-f430-4dfc-a786-edf49bfda810_200x200-xs.png" loading="lazy" alt="Unsupervised Learning"><div class="u-small">Previous Post<h5>Day 4 Unsupervised Learning</h5></div></a></div><div class="post__nav__next"><a href="https://aayushhh.github.io/blogss/day-6-learning-rate.html" class="post__nav__link" rel="next"><div class="u-small">Next Post<h5>Day 6 Learning Rate</h5></div><img src="https://aayushhh.github.io/blogss/media/posts/7/responsive/72f1695b-f430-4dfc-a786-edf49bfda810_200x200-xs.png" loading="lazy" alt=""></a></div></nav></footer></article><div class="comments box"><h3 class="box__title">Comments</h3><div id="disqus_thread"></div><script>var disqus_config = function () {
                    this.page.url = 'https://aayushhh.github.io/blogss/day-6-trainning-andloss-reducing-loss.html';
            		this.page.identifier = '6';
                };
            
                var disqus_loaded = false;
            
                function publiiLoadDisqus() {
                    if(disqus_loaded) {
                        return false;
                    }
            
                    var top = document.getElementById('disqus_thread').offsetTop;
            
                    if (!disqus_loaded && (window.scrollY || window.pageYOffset) + window.innerHeight > top) {
                        disqus_loaded = true;
            
                        (function () {
                            var d = document, s = d.createElement('script');
                            s.src = 'https://.disqus.com/embed.js';
                            s.setAttribute('data-timestamp', +new Date());
                            (d.head || d.body).appendChild(s);
                        })();
                    }
                }
            
                publiiLoadDisqus();
            
                window.onscroll = function() {
                    publiiLoadDisqus();
                };</script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript></div></main><div class="sidebar"><section class="box"><h3 class="box__title">Follow us</h3><div class="follow"><a href="https://www.facebook.com/aayushtekken" class="facebook" aria-label="Facebook"><svg class="u-icon"><use xlink:href="https://aayushhh.github.io/blogss/assets/svg/svg-map.svg#facebook"/></svg> Facebook </a><a href="https://twitter.com/aayusshhhhhh" class="twitter" aria-label="Twitter"><svg class="u-icon"><use xlink:href="https://aayushhh.github.io/blogss/assets/svg/svg-map.svg#twitter"/></svg> Twitter </a><a href="https://www.instagram.com/aayushhhhhh/" class="instagram" aria-label="Instagram"><svg class="u-icon"><use xlink:href="https://aayushhh.github.io/blogss/assets/svg/svg-map.svg#instagram"/></svg> Instagram </a><a href="https://www.linkedin.com/in/aayush-r-44a33a108" class="linkedin" aria-label="LinkedIn"><svg class="u-icon"><use xlink:href="https://aayushhh.github.io/blogss/assets/svg/svg-map.svg#linkedin"/></svg> LinkedIn </a><a href="https://www.youtube.com/channel/UCy-YJwQ2k3ZrAkKzeuAemhA/videos?view_as&#x3D;subscriber" class="youtube" aria-label="Youtube"><svg class="u-icon"><use xlink:href="https://aayushhh.github.io/blogss/assets/svg/svg-map.svg#youtube"/></svg> Youtube</a></div></section><section class="box authors"><h3 class="box__title">Authors</h3><ul class="authors__cotainer"><li class="authors__item"><div><a href="https://aayushhh.github.io/blogss/authors/aayush/" class="authors__title">Aayush</a> <span class="u-small">Post: 9</span></div></li></ul></section><section class="newsletter box box--gray"><h3 class="box__title">Newsletter</h3><p class="newsletter__description">Sign up to receive email updates and to hear what's going on with us!</p><form>...</form></section><section class="box"><h3 class="box__title">Tags</h3><ul class="tags"><li><a href="https://aayushhh.github.io/blogss/tags/31daysofml/">31daysofML</a> <span class="u-small">(6)</span></li><li><a href="https://aayushhh.github.io/blogss/tags/ai/">AI</a> <span class="u-small">(6)</span></li><li><a href="https://aayushhh.github.io/blogss/tags/appimage/">AppImage</a> <span class="u-small">(1)</span></li><li><a href="https://aayushhh.github.io/blogss/tags/artificial-intelligence/">Artificial Intelligence</a> <span class="u-small">(3)</span></li><li><a href="https://aayushhh.github.io/blogss/tags/building-model/">Building Model</a> <span class="u-small">(1)</span></li></ul></section></div><footer class="footer"><a class="footer__logo" href="https://aayushhh.github.io/blogss/">Blogs</a><nav><ul class="footer__nav"></ul></nav><div class="footer__follow"><a href="https://www.facebook.com/aayushtekken" aria-label="Facebook"><svg><use xlink:href="https://aayushhh.github.io/blogss/assets/svg/svg-map.svg#facebook"/></svg> </a><a href="https://twitter.com/aayusshhhhhh" aria-label="Twitter"><svg><use xlink:href="https://aayushhh.github.io/blogss/assets/svg/svg-map.svg#twitter"/></svg> </a><a href="https://www.instagram.com/aayushhhhhh/" aria-label="Instagram"><svg><use xlink:href="https://aayushhh.github.io/blogss/assets/svg/svg-map.svg#instagram"/></svg> </a><a href="https://www.linkedin.com/in/aayush-r-44a33a108" aria-label="LinkedIn"><svg><use xlink:href="https://aayushhh.github.io/blogss/assets/svg/svg-map.svg#linkedin"/></svg> </a><a href="https://www.youtube.com/channel/UCy-YJwQ2k3ZrAkKzeuAemhA/videos?view_as&#x3D;subscriber" aria-label="Youtube"><svg><use xlink:href="https://aayushhh.github.io/blogss/assets/svg/svg-map.svg#youtube"/></svg></a></div><div class="footer__copyright"><p>Made By <a href="http://www.aayushrajput.site" target="_blank" rel="noopener">Aayush Rajput</a></p></div></footer></div><script>window.publiiThemeMenuConfig = {    
        mobileMenuMode: 'sidebar',
        animationSpeed: 300,
        submenuWidth: 'auto',
        doubleClickTime: 500,
        mobileMenuExpandableSubmenus: true, 
        relatedContainerForOverlayMenuSelector: '.navbar',
   };</script><script defer="defer" src="https://aayushhh.github.io/blogss/assets/js/scripts.min.js?v=3dc768003cc6d66c9e7bf0904e71d3e8"></script><script>var images = document.querySelectorAll('img[loading]');

      for (var i = 0; i < images.length; i++) {
         if (images[i].complete) {
               images[i].classList.add('is-loaded');
         } else {
               images[i].addEventListener('load', function () {
                  this.classList.add('is-loaded');
               }, false);
         }
      }</script><script id="dsq-count-scr" src="https://.disqus.com/count.js" async></script></body></html>